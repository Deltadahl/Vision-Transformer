{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vision Transformer - Explanation and Implementation\n",
    "This notebook explains how the Vision Tranformer (ViT) work, how to implement it and train it from scratch in PyTorch.\n",
    "\n",
    "The dataset that is used is the **Tiny ImageNet** containing $100000$ images of $200$ classes so that the model can be trained on a single GPU in reasonable time.\n",
    "#TODO move this further down? test with a geater dataset?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep learning imports\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as T\n",
    "import wandb\n",
    "# import timm # TODO remove if not used\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import make_grid\n",
    "from einops import rearrange\n",
    "\n",
    "# Other imports\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does the ViT work? \n",
    "Convolutional neural networks (CNN) dominated the field of computer vision in the yeards 2012-2020. But in 2020 the paper [An Image is Worth 16x16 Words](https://arxiv.org/abs/2010.11929) showed that ViT can attain state-of-the-art (SOTA) result with less computational cost.\n",
    "\n",
    "The arcitecture of the ViT is shown in Figure 1. \n",
    "A 2D image is split into a number of patches e.g. 9 2D patches. Each patch is flattened and maped with a linear projection. \n",
    "The output of this mapping is concatinated with an extra learnable class [cls] embedding. The state of the [cls] embedding is randomly initialized, but it will accumulate information from the other tokens in the transformer and is used as the output of the transformer.\n",
    "\n",
    "\n",
    "Unlike a CNN, a ViT have no inherent way to retrieve position from its input. Therefore a positional embedding is introduced. It could be concatinated with all embedded patches, but that comes with a computational cost, therefore the positional embedding is added to the embedded patches, whitch empirically gives good results [(Dosovitskiy et al., 2020)](https://arxiv.org/abs/2010.11929).\n",
    "After the positional encoding is added the embedded patches is fed into the **Transformer encoder**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Figures/Vit_fig_from_paper.png\" width=\"800\"> \n",
    "\n",
    "Figure 1: Model overview [[1]](https://arxiv.org/abs/2010.11929)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "    def __init__(self, *, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, channels=3, dropout=0.1, stochastic_depth_prob=0):\n",
    "        super().__init__()\n",
    "        assert image_size % patch_size == 0, 'image size must be divisible by the patch size'\n",
    "        num_patches = (image_size // patch_size) ** 2\n",
    "        patch_dim = channels * patch_size ** 2\n",
    "\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n",
    "        self.patch_to_embedding = nn.Linear(patch_dim, dim)\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
    "        self.transformer = Transformer(dim, depth, heads, mlp_dim, dropout, stochastic_depth_prob)\n",
    "\n",
    "        self.to_cls_token = nn.Identity()\n",
    "\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.Linear(dim, mlp_dim),\n",
    "            # TODO dropout here?\n",
    "            # GELU is an activation function similar to RELU, but have been shown to improve the performance https://arxiv.org/pdf/1606.08415v4.pdf\n",
    "            nn.GELU(),\n",
    "            nn.Linear(mlp_dim, num_classes)\n",
    "        )\n",
    "        self.init_params()\n",
    "\n",
    "\n",
    "    # Xavier initialization of parameters. This can help, but I didn't see that big of an impact in this case.\n",
    "    # TODO remove?\n",
    "    def init_params(self):\n",
    "        for name, p in self.named_parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "\n",
    "\n",
    "    def forward(self, img, training = True):\n",
    "        p = self.patch_size\n",
    "\n",
    "        x = rearrange(img, 'b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = p, p2 = p)\n",
    "        x = self.patch_to_embedding(x)\n",
    "\n",
    "        # TODO check this, copy the cls-token batchsize times\n",
    "        cls_tokens = self.cls_token.expand(img.shape[0], -1, -1)\n",
    "        # prepend the cls token to every image\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        # TODO check if pos-embedding << x\n",
    "        x += self.pos_embedding\n",
    "        x = self.transformer(x, training)\n",
    "\n",
    "        # Only uses the cls-token to classify the image\n",
    "        x = self.to_cls_token(x[:, 0])\n",
    "        x = torch.sigmoid(x)\n",
    "        return self.mlp_head(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Transformer Encoder\n",
    "The ViT uses the encoder introduced in the famous [Attention Is All You Need](https://arxiv.org/abs/1706.03762?context=cs) paper, see Figure 1.\n",
    "The encoder consists of two blocks, a multiheaded self-attention and a multilayer perceptron. Before each block a [layernorm](https://arxiv.org/abs/1607.06450) is applied and each block is surounded by a [residual connection](https://arxiv.org/abs/1512.03385). A residual connection not needed in theory, but empirically it is found to make a big differance. The residual connection can help the network to learn a desired mapping $H(x)$ by instead letting the network fit another mapping $F(x) := H(x) - x$ and then add $x$ to $F(x)$ to get the desired $H(x)$. \n",
    "\n",
    "The self-attention used here is a simple function of three matrices $Q, K, V$ (queries, keys, and values)\n",
    "\\begin{equation}\n",
    "\\text{Attention}(Q,K,V)=\\text{softmax}(\\frac{QK^{\\top}}{\\sqrt{d_k}})V,\n",
    "\\end{equation}\n",
    "where the scaling factor $d_k$ is the dimension of the queries and keys.\n",
    "\n",
    "#TODO continue\n",
    "Instead of performing a single attention function with $d_{model}$-dimensional queries, keys and values, multiheaded self-attention performes three linear projections to $d_k$, $d_k$ and $d_v$ dimensions respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, dim, depth, heads, mlp_dim, dropout, stochastic_depth_prob_rate_last):\n",
    "        super().__init__()\n",
    "        self.depth = depth\n",
    "        self.layers = nn.ModuleList([])\n",
    "        self.stochastic_depth_prob_rate_last = stochastic_depth_prob_rate_last\n",
    "        for _ in range(depth):\n",
    "\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                Residual(PreNorm(dim, Attention(dim, heads = heads))),\n",
    "                Residual(PreNorm(dim, FeedForward(dim, mlp_dim, dropout)))\n",
    "            ]))\n",
    "\n",
    "    def forward(self, x, training):\n",
    "        d  = self.depth - 1\n",
    "        for layer_num, (attn, ff) in enumerate(self.layers):\n",
    "            \n",
    "            # Stochastic depth probability implementation\n",
    "            if self.depth > 1 and training:\n",
    "                prob_to_skip = self.stochastic_depth_prob_rate_last*layer_num/(self.depth-1)\n",
    "                rand_num = np.random.rand()\n",
    "                if rand_num < prob_to_skip:\n",
    "                    return x\n",
    "\n",
    "            x = attn(x)\n",
    "            x = ff(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads=8):\n",
    "        super().__init__()\n",
    "        self.heads = heads\n",
    "        self.scale = dim ** -0.5\n",
    "\n",
    "        self.to_qkv = nn.Linear(dim, dim * 3, bias=False)\n",
    "        self.to_out = nn.Linear(dim, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, n, _, h = *x.shape, self.heads\n",
    "        qkv = self.to_qkv(x)\n",
    "        q, k, v = rearrange(qkv, 'b n (qkv h d) -> qkv b h n d', qkv=3, h=h)\n",
    "\n",
    "        dots = torch.einsum('bhid,bhjd->bhij', q, k) * self.scale\n",
    "\n",
    "        attn = dots.softmax(dim=-1)\n",
    "\n",
    "        out = torch.einsum('bhij,bhjd->bhid', attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        out =  self.to_out(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Residual(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(x, **kwargs) + x\n",
    "\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(self.norm(x), **kwargs)\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Linear(dim, hidden_dim)\n",
    "        self.l2 = nn.Linear(hidden_dim, dim)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.l1(x)\n",
    "        x = F.dropout(x, self.dropout)\n",
    "        x = F.gelu(x)\n",
    "        x = self.l2(x)\n",
    "        x = F.dropout(x, self.dropout)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class RandomMixup(torch.nn.Module):\n",
    "    \"\"\"Randomly apply Mixup to the provided batch and targets.\n",
    "    The class implements the data augmentations as described in the paper\n",
    "    d`\"mixup: Beyond Empirical Risk Minimization\" <https://arxiv.org/abs/1710.09412>`_.\n",
    "    Args:\n",
    "        num_classes (int): number of classes used for one-hot encoding.\n",
    "        p (float): probability of the batch being transformed. Default value is 0.5.\n",
    "        alpha (float): hyperparameter of the Beta distribution used for mixup.\n",
    "            Default value is 1.0.\n",
    "        inplace (bool): boolean to make this transform inplace. Default set to False.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_classes: int, p: float = 0.5, alpha: float = 1.0, inplace: bool = False) -> None:\n",
    "        super().__init__()\n",
    "        assert num_classes > 0, \"Please provide a valid positive value for the num_classes.\"\n",
    "        assert alpha > 0, \"Alpha param can't be zero.\"\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.p = p\n",
    "        self.alpha = alpha\n",
    "        self.inplace = inplace\n",
    "\n",
    "    def forward(self, batch, target):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            batch (Tensor): Float tensor of size (B, C, H, W)\n",
    "            target (Tensor): Integer tensor of size (B, )\n",
    "        Returns:\n",
    "            Tensor: Randomly transformed batch.\n",
    "        \"\"\"\n",
    "        if batch.ndim != 4:\n",
    "            raise ValueError(f\"Batch ndim should be 4. Got {batch.ndim}\")\n",
    "        if target.ndim != 1:\n",
    "            raise ValueError(f\"Target ndim should be 1. Got {target.ndim}\")\n",
    "        if not batch.is_floating_point():\n",
    "            raise TypeError(f\"Batch dtype should be a float tensor. Got {batch.dtype}.\")\n",
    "        if target.dtype != torch.int64:\n",
    "            raise TypeError(f\"Target dtype should be torch.int64. Got {target.dtype}\")\n",
    "\n",
    "        if not self.inplace:\n",
    "            batch = batch.clone()\n",
    "            target = target.clone()\n",
    "\n",
    "        if target.ndim == 1:\n",
    "            target = torch.nn.functional.one_hot(target, num_classes=self.num_classes).to(dtype=batch.dtype)\n",
    "\n",
    "        if torch.rand(1).item() >= self.p:\n",
    "            return batch, target\n",
    "        \n",
    "        # It's faster to roll the batch by one instead of shuffling it to create image pairs\n",
    "        batch_rolled = batch.roll(1, 0)\n",
    "        target_rolled = target.roll(1, 0)\n",
    "\n",
    "        # Implemented as on mixup paper, page 3.\n",
    "        lambda_param = float(torch._sample_dirichlet(torch.tensor([self.alpha, self.alpha]))[0])\n",
    "        batch_rolled.mul_(1.0 - lambda_param)\n",
    "        batch.mul_(lambda_param).add_(batch_rolled)\n",
    "\n",
    "        target_rolled.mul_(1.0 - lambda_param)\n",
    "        target.mul_(lambda_param).add_(target_rolled)\n",
    "\n",
    "        return batch, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:njc42gs8) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">devoted-sky-15</strong>: <a href=\"https://wandb.ai/simonc/ViT-ImageNet1k/runs/njc42gs8\" target=\"_blank\">https://wandb.ai/simonc/ViT-ImageNet1k/runs/njc42gs8</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20220907_065017-njc42gs8\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:njc42gs8). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\ccsim\\OneDrive\\AI_neerd\\ViT-ImageNet-Tiny\\wandb\\run-20220907_065219-10di1r7a</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/simonc/ViT-ImageNet1k/runs/10di1r7a\" target=\"_blank\">misunderstood-waterfall-16</a></strong> to <a href=\"https://wandb.ai/simonc/ViT-ImageNet1k\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on the GPU\n",
      "Running on the CPU\n",
      "Epoch: 1\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ccsim\\OneDrive\\AI_neerd\\ViT-ImageNet-Tiny\\ViT.ipynb Cell 10\u001b[0m in \u001b[0;36m<cell line: 140>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/ccsim/OneDrive/AI_neerd/ViT-ImageNet-Tiny/ViT.ipynb#X12sZmlsZQ%3D%3D?line=140'>141</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mEpoch:\u001b[39m\u001b[39m'\u001b[39m, epoch)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/ccsim/OneDrive/AI_neerd/ViT-ImageNet-Tiny/ViT.ipynb#X12sZmlsZQ%3D%3D?line=141'>142</a>\u001b[0m start_time_epoch \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/ccsim/OneDrive/AI_neerd/ViT-ImageNet-Tiny/ViT.ipynb#X12sZmlsZQ%3D%3D?line=142'>143</a>\u001b[0m loss_train \u001b[39m=\u001b[39m train_epoch(model, optimizer, train_loader)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/ccsim/OneDrive/AI_neerd/ViT-ImageNet-Tiny/ViT.ipynb#X12sZmlsZQ%3D%3D?line=143'>144</a>\u001b[0m loss_val, acc_val \u001b[39m=\u001b[39m evaluate(model, val_loader)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/ccsim/OneDrive/AI_neerd/ViT-ImageNet-Tiny/ViT.ipynb#X12sZmlsZQ%3D%3D?line=144'>145</a>\u001b[0m wandb\u001b[39m.\u001b[39mlog({\u001b[39m\"\u001b[39m\u001b[39mloss train\u001b[39m\u001b[39m\"\u001b[39m: loss_train, \u001b[39m\"\u001b[39m\u001b[39mloss val\u001b[39m\u001b[39m\"\u001b[39m: loss_val, \u001b[39m\"\u001b[39m\u001b[39macc val\u001b[39m\u001b[39m\"\u001b[39m: acc_val, \u001b[39m\"\u001b[39m\u001b[39mTime for epoch\u001b[39m\u001b[39m\"\u001b[39m: (time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_time_epoch)})\n",
      "\u001b[1;32mc:\\Users\\ccsim\\OneDrive\\AI_neerd\\ViT-ImageNet-Tiny\\ViT.ipynb Cell 10\u001b[0m in \u001b[0;36mtrain_epoch\u001b[1;34m(model, optimizer, data_loader)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ccsim/OneDrive/AI_neerd/ViT-ImageNet-Tiny/ViT.ipynb#X12sZmlsZQ%3D%3D?line=75'>76</a>\u001b[0m loss \u001b[39m=\u001b[39m loss_fun(output, target)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ccsim/OneDrive/AI_neerd/ViT-ImageNet-Tiny/ViT.ipynb#X12sZmlsZQ%3D%3D?line=76'>77</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/ccsim/OneDrive/AI_neerd/ViT-ImageNet-Tiny/ViT.ipynb#X12sZmlsZQ%3D%3D?line=77'>78</a>\u001b[0m optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ccsim/OneDrive/AI_neerd/ViT-ImageNet-Tiny/ViT.ipynb#X12sZmlsZQ%3D%3D?line=78'>79</a>\u001b[0m total_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ccsim/OneDrive/AI_neerd/ViT-ImageNet-Tiny/ViT.ipynb#X12sZmlsZQ%3D%3D?line=79'>80</a>\u001b[0m \u001b[39mif\u001b[39;00m i \u001b[39m%\u001b[39m \u001b[39m20\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\ccsim\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\optim\\optimizer.py:113\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    111\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[0;32m    112\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[1;32m--> 113\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ccsim\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\autograd\\grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[1;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ccsim\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\optim\\adamw.py:114\u001b[0m, in \u001b[0;36mAdamW.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[39m@torch\u001b[39m\u001b[39m.\u001b[39mno_grad()\n\u001b[0;32m    107\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, closure\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    108\u001b[0m     \u001b[39m\"\"\"Performs a single optimization step.\u001b[39;00m\n\u001b[0;32m    109\u001b[0m \n\u001b[0;32m    110\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m    111\u001b[0m \u001b[39m        closure (callable, optional): A closure that reevaluates the model\u001b[39;00m\n\u001b[0;32m    112\u001b[0m \u001b[39m            and returns the loss.\u001b[39;00m\n\u001b[0;32m    113\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cuda_graph_capture_health_check()\n\u001b[0;32m    116\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    117\u001b[0m     \u001b[39mif\u001b[39;00m closure \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\ccsim\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\optim\\optimizer.py:86\u001b[0m, in \u001b[0;36mOptimizer._cuda_graph_capture_health_check\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_cuda_graph_capture_health_check\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m     85\u001b[0m     \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mhas_cuda \u001b[39mand\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available():\n\u001b[1;32m---> 86\u001b[0m         capturing \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mcuda\u001b[39m.\u001b[39;49mis_current_stream_capturing()\n\u001b[0;32m     88\u001b[0m         \u001b[39mif\u001b[39;00m capturing \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mcapturable\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[0;32m     89\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mAttempting CUDA graph capture of step() for an instance of \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m\n\u001b[0;32m     90\u001b[0m                                \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39m+\u001b[39m\n\u001b[0;32m     91\u001b[0m                                \u001b[39m\"\u001b[39m\u001b[39m but this instance was constructed with capturable=False.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ccsim\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\cuda\\graphs.py:24\u001b[0m, in \u001b[0;36mis_current_stream_capturing\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mis_current_stream_capturing\u001b[39m():\n\u001b[0;32m     19\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[39m    Returns True if CUDA graph capture is underway on the current CUDA stream, False otherwise.\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \n\u001b[0;32m     22\u001b[0m \u001b[39m    If a CUDA context does not exist on the current device, returns False without initializing the context.\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m     \u001b[39mreturn\u001b[39;00m _cuda_isCurrentStreamCapturing()\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "wandb.init(project=\"ViT-ImageNet1k\")\n",
    "config = wandb.config\n",
    "#torch.manual_seed(42)\n",
    "\n",
    "scaling_factor = 1\n",
    "LR = 3e-3/(scaling_factor)\n",
    "BATCH_SIZE_TRAIN = 1024//scaling_factor\n",
    "BATCH_SIZE_VAL = 1024//scaling_factor\n",
    "N_EPOCHS = 10\n",
    "DROPOUT = 0.1\n",
    "WEIGHT_DECAY = 0.01\n",
    "NUM_CLASSES = 2\n",
    "IMAGE_SIZE = 256\n",
    "#DATA_DIR = 'tiny-imagenet-200' # Original images come in shapes of [3,64,64]\n",
    "DATA_DIR = 'ImageNet1k-2'\n",
    "\n",
    "config.lr = LR\n",
    "config.batch_size_train = BATCH_SIZE_TRAIN\n",
    "config.batch_size_val = BATCH_SIZE_VAL\n",
    "config.n_epochs = N_EPOCHS\n",
    "config.dropout = DROPOUT\n",
    "config.weight_decay = WEIGHT_DECAY\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    print('Running on the GPU')\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print('Running on the CPU')\n",
    "device = torch.device(\"cpu\")\n",
    "print('Running on the CPU')\n",
    "\n",
    "# Define training and validation data paths\n",
    "TRAIN_DIR = os.path.join(DATA_DIR, 'train') \n",
    "#VALID_DIR = os.path.join(DATA_DIR, r'val/images')\n",
    "VALID_DIR = os.path.join(DATA_DIR, 'train') \n",
    "\n",
    "transforms_train = T.Compose([\n",
    "                T.Resize((256, 256)), # TODO\n",
    "                T.RandomHorizontalFlip(),\n",
    "                T.RandAugment(),\n",
    "                T.ToTensor(),\n",
    "                T.Normalize(mean=[0.5, 0.5, 0.5], \n",
    "                               std=[0.25, 0.25, 0.25]),             \n",
    "])\n",
    "transforms_val = T.Compose([\n",
    "                T.Resize((256, 256)), # TODO\n",
    "                T.ToTensor(),\n",
    "                T.Normalize(mean=[0.5, 0.5, 0.5], \n",
    "                               std=[0.25, 0.25, 0.25]),             \n",
    "])\n",
    "         \n",
    "data_train = datasets.ImageFolder(TRAIN_DIR, transform=transforms_train)\n",
    "train_loader = DataLoader(data_train, batch_size=BATCH_SIZE_TRAIN, shuffle=True)\n",
    "data_val = datasets.ImageFolder(VALID_DIR, transform=transforms_val)\n",
    "val_loader = DataLoader(data_val, batch_size=BATCH_SIZE_VAL, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "mixup = RandomMixup(num_classes=NUM_CLASSES)\n",
    "\n",
    "def train_epoch(model, optimizer, data_loader):\n",
    "    total_samples = len(data_loader.dataset)\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for i, (data, target) in enumerate(data_loader):\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        (data, target) = mixup(data, target)     \n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        #loss = F.nll_loss(output, target)\n",
    "        loss = loss_fun(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        if i % 20 == 0:\n",
    "            print('[' +  '{:5}'.format(i * len(data)) + '/' + '{:5}'.format(total_samples) +\n",
    "                  ' (' + '{:3.0f}'.format(100 * i / len(data_loader)) + '%)]  Loss: ' +\n",
    "                  '{:6.4f}'.format(loss.item()))\n",
    "    avg_loss = 1000*total_loss / total_samples\n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "def evaluate(model, data_loader):\n",
    "    model.eval()\n",
    "\n",
    "    total_samples = len(data_loader.dataset)\n",
    "    correct_samples = 0\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in data_loader:\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "            #output = F.log_softmax(model(data, training=False), dim=1)\n",
    "            # TODO change back when ResNet18 is not used\n",
    "            output = F.log_softmax(model(data), dim=1)\n",
    "            loss = F.nll_loss(output, target, reduction='sum')\n",
    "            _, pred = torch.max(output, dim=1)\n",
    "            total_loss += loss.item()\n",
    "            correct_samples += pred.eq(target).sum()\n",
    "\n",
    "    avg_loss = total_loss / total_samples\n",
    "    acc = 100*(correct_samples / total_samples)\n",
    "    print('Average val loss: ' + '{:.4f}'.format(avg_loss) +\n",
    "          '  Accuracy:' + '{:5}'.format(correct_samples) + '/' +\n",
    "          '{:5}'.format(total_samples) + ' (' +\n",
    "          '{:4.2f}'.format(100.0 * correct_samples / total_samples) + '%)')\n",
    "    return avg_loss, acc\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "#model = ViT(image_size=64, patch_size=8, num_classes=NUM_CLASSES, channels=3,\n",
    "#            dim=64, depth=16, heads=8, mlp_dim=128, dropout=DROPOUT).to(device)\n",
    "# TODO\n",
    "\n",
    "model = ViT(image_size=IMAGE_SIZE, patch_size=IMAGE_SIZE//8, num_classes=NUM_CLASSES, channels=3,\n",
    "            dim=64, depth=1, heads=8, mlp_dim=128, dropout=DROPOUT).to(device)\n",
    "#from ResNet import ResNet18\n",
    "#model = ResNet18(num_classes=200, dropout=0.0).to(device)\n",
    "\n",
    "wandb.watch(model)\n",
    "\n",
    "# Load model\n",
    "path_to_model_load = r'Models\\37_77__2.6530152587890625.pt'\n",
    "load_model = False\n",
    "if os.path.exists(path_to_model_load) and load_model:\n",
    "    print('Loading model.')\n",
    "    model.load_state_dict(torch.load(path_to_model_load))\n",
    "    model.train()\n",
    "\n",
    "loss_fun = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "min_loss_val = np.inf\n",
    "for epoch in range(1, N_EPOCHS + 1):\n",
    "    print('Epoch:', epoch)\n",
    "    start_time_epoch = time.time()\n",
    "    loss_train = train_epoch(model, optimizer, train_loader)\n",
    "    loss_val, acc_val = evaluate(model, val_loader)\n",
    "    wandb.log({\"loss train\": loss_train, \"loss val\": loss_val, \"acc val\": acc_val, \"Time for epoch\": (time.time() - start_time_epoch)})\n",
    "    print('Execution time for Epoch:', '{:5.2f}'.format(time.time() - start_time_epoch), 'seconds')\n",
    "    if min_loss_val > loss_val and epoch:\n",
    "        min_loss_val = loss_val\n",
    "        path_to_model_save = r'Models/min_loss_ResNet' + str(loss_val) + \".pt\"\n",
    "        torch.save(model.state_dict(), path_to_model_save)\n",
    "    \n",
    "print('Execution time:', '{:5.2f}'.format(time.time() - start_time), 'seconds\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to display single or a batch of sample images\n",
    "def imshow(img):\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "    \n",
    "def show_batch(dataloader):\n",
    "    dataiter = iter(dataloader)\n",
    "    images, labels = dataiter.next()    \n",
    "    imshow(make_grid(images))\n",
    "    \n",
    "def show_image(dataloader):\n",
    "    dataiter = iter(dataloader)\n",
    "    images, labels = dataiter.next()\n",
    "    random_num = random.randint(0, len(images)-1)\n",
    "    imshow(images[random_num])\n",
    "    label = labels[random_num]\n",
    "    print(f'Label: {label}, Shape: {images[random_num].shape}')\n",
    "\n",
    "#show_batch(train_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3c6c179630d9cb3bbc2ff618954a106dca7f0a81035ad1531e0f1d9ca655ea36"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
